{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 5\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what is given activation function(sigma)?\n",
    "def activate(x):\n",
    "    return x\n",
    "\n",
    "def grad_of_activate(x):\n",
    "    return x\n",
    "\n",
    "# helper functions\n",
    "def compute_layer(x, W, b):\n",
    "    return np.add(np.multiply(np.transpose(W), x), b)\n",
    "\n",
    "def compute_two_layers(x, W1, b1, W2, b2):\n",
    "    return compute_layer(compute_layer(x, W1, b1), W2, b2)\n",
    "\n",
    "def grad_shared_part(v, y, W, b, j):\n",
    "    \"\"\" v can be x or N1(x) \"\"\"\n",
    "    tmp = np.add(np.multiply(np.transpose(W)[j,:], v), b[j,:])\n",
    "    return activate(tmp - y[j,:]) * grad_of_activate(tmp)\n",
    "\n",
    "def grads_per_sample(x, y, W1, b1, W2, b2):\n",
    "    L1_output = compute_layer(x, W1, b1)\n",
    "    \n",
    "    # init\n",
    "    grad_W1, grad_b1, grad_W2, grad_b2 = \\ \n",
    "        np.zeros_like(W1), np.zeros_like(b1), np.zeros_like(W2), np.zeros_like(b2)\n",
    "    \n",
    "    # vectorize this? \n",
    "    for j in W1.shape[0]:\n",
    "        tmp = grad_shared_part(L1_output, y, W2, b2, j)\n",
    "        grad_b2[j] += tmp\n",
    "        grad_b1[j] += tmp * np.transpose(W2)[j,k]\n",
    "        \n",
    "        for k in W1.shape[1]:\n",
    "            grad_W2[j,k] = tmp * L1_output[k,:]\n",
    "            grad_W1[j,k] = grad_b1 + x[k,:]\n",
    "            \n",
    "    return (grad_W1, grad_b1, grad_W2, grad_b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (iii) two layer neural network\n",
    "def train_two_layer_nn(data, k, step_size):\n",
    "    # dimensions\n",
    "    I1, O1, I2, O2 = 1, k, k, 1\n",
    "    \n",
    "    # init params for each layer\n",
    "    W1 = np.random.rand(I1, O1)\n",
    "    b1 = np.random.rand(O1, 1)\n",
    "    W2 = np.random.rand(I2, O2)\n",
    "    b2 = np.random.rand(O2, 1)\n",
    "    \n",
    "    # iter for termination\n",
    "    it, max_iter = 0, 10000\n",
    "    while it < max_iter: \n",
    "        # break if convergence\n",
    "        \n",
    "        for sample in data:\n",
    "            # parse x,y\n",
    "            x, y = sample[]\n",
    "            \n",
    "            # compute y_hat. why?\n",
    "            y_hat = compute_two_layers(x, W1, b1, W2, b2)\n",
    "            \n",
    "            # compute grads\n",
    "            g = grads_per_sample(x, y, W1, b1, W2, b2)\n",
    "            \n",
    "            # update parameters\n",
    "            W1 -= step_size * g[0]\n",
    "            b1 -= step_size * g[1]\n",
    "            W2 -= step_size * g[2]\n",
    "            b2 -= step_size * g[3]\n",
    "    \n",
    "    return (W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (iv)\n",
    "\n",
    "# open, read, and concat X,Y\n",
    "\n",
    "# train\n",
    "W1, b1, W2, b2 = train_two_layer_nn(data, k, step_size)\n",
    "\n",
    "for sample in data:\n",
    "    x = sample[] # parse x \n",
    "    y_hat = compute_two_layers(x, W1, b1, W2, b2)\n",
    "    # plot x vs y_hat, y per sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
